Directory structure:
‚îî‚îÄ‚îÄ pinpong/
    ‚îú‚îÄ‚îÄ agent.py
    ‚îú‚îÄ‚îÄ ball.py
    ‚îú‚îÄ‚îÄ config.py
    ‚îú‚îÄ‚îÄ game.py
    ‚îú‚îÄ‚îÄ main_visual.py
    ‚îú‚îÄ‚îÄ neural_network.py
    ‚îú‚îÄ‚îÄ paddle.py
    ‚îú‚îÄ‚îÄ pyproject.toml
    ‚îî‚îÄ‚îÄ vtrace_agent.py
================================================
FILE: agent.py
================================================
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque, namedtuple
import os
# Internal project imports
import config
from neural_network import DQN
# --- Device Setup ---
# Set the device to GPU if available, otherwise CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Use a named tuple to represent a single transition in our environment for clarity
Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))
class Agent:
    """
    Implements a Deep Q-Network (DQN) agent for playing the Pong game.
    This class encapsulates the neural networks (policy and target), the experience
    replay memory, the action selection policy (epsilon-greedy), and the learning
    algorithm.
    """
    def __init__(self, agent_id: str):
        """
        Initializes the DQN Agent.
        Args:
            agent_id (str): A unique identifier for the agent (e.g., 'agent1' or 'agent2').
                            Used for saving and loading models.
        """
        self.agent_id = agent_id
        self.state_size = config.STATE_SIZE
        self.action_size = config.ACTION_SIZE
        # --- Q-Network Initialization ---
        # Policy Network: The network we are actively training
        self.policy_net = DQN(self.state_size, self.action_size).to(device)
        # Target Network: A clone of the policy network for stabilizing learning
        self.target_net = DQN(self.state_size, self.action_size).to(device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()  # Set target network to evaluation mode
        # --- Optimizer and Loss Function ---
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=config.LEARNING_RATE)
        # Smooth L1 Loss is often more stable than MSE for Q-learning
        self.loss_fn = nn.SmoothL1Loss()
        # --- Replay Memory ---
        # A deque (double-ended queue) for storing experiences efficiently
        self.memory = deque(maxlen=config.MEMORY_SIZE)
        # --- Epsilon-Greedy Policy ---
        # Epsilon is the probability of choosing a random action (exploration)
        self.epsilon = config.EPSILON_START
    def choose_action(self, state: np.ndarray) -> int:
        """
        Selects an action using an epsilon-greedy policy.
        With probability epsilon, a random action is chosen (exploration).
        Otherwise, the action with the highest Q-value predicted by the policy
        network is chosen (exploitation).
        Args:
            state (np.ndarray): The current state of the environment.
        Returns:
            int: The chosen action (0: STAY, 1: UP, 2: DOWN, 3: SWING).
        """
        if random.random() < self.epsilon:
            # Exploration: choose a random action
            return random.randrange(self.action_size)
        else:
            # Exploitation: choose the best action based on the policy network
            state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)
            self.policy_net.eval()  # Set network to evaluation mode for inference
            with torch.no_grad():
                action_values = self.policy_net(state_tensor)
            self.policy_net.train() # Set it back to train mode for subsequent learning
            # Get the action with the highest Q-value
            return torch.argmax(action_values).item()
    def remember(self, state, action, reward, next_state, done):
        """
        Stores an experience tuple in the replay memory.
        Args:
            state (np.ndarray): The starting state.
            action (int): The action taken.
            reward (float): The reward received.
            next_state (np.ndarray): The resulting state.
            done (bool): A flag indicating if the episode has ended.
        """
        experience = Experience(state, action, reward, next_state, done)
        self.memory.append(experience)
    def learn(self):
        """
        Trains the policy network using a batch of experiences from the replay memory.
        This method samples a minibatch, calculates the target Q-values using the
        target network, computes the loss against the policy network's predictions,
        and performs a gradient descent step.
        """
        if len(self.memory) < config.BATCH_SIZE:
            return  # Not enough experiences in memory to train
        # Sample a random minibatch of experiences from memory
        experiences = random.sample(self.memory, config.BATCH_SIZE)
        batch = Experience(*zip(*experiences))
        # Convert batch of experiences to PyTorch tensors
        states = torch.from_numpy(np.vstack(batch.state)).float().to(device)
        actions = torch.from_numpy(np.vstack(batch.action)).long().to(device)
        rewards = torch.from_numpy(np.vstack(batch.reward)).float().to(device)
        next_states = torch.from_numpy(np.vstack(batch.next_state)).float().to(device)
        dones = torch.from_numpy(np.vstack(batch.done).astype(np.uint8)).float().to(device)
        # 1. Get Q-values for current states from the policy network
        # We need to select the Q-value for the action that was actually taken.
        # policy_net(states) -> (batch_size, action_size)
        # .gather(1, actions) -> selects the specific action's Q-value for each state
        current_q_values = self.policy_net(states).gather(1, actions)
        # 2. Calculate target Q-values for the next states from the target network
        with torch.no_grad(): # We don't need gradients for the target network
            # .max(1)[0] gives the max Q-value for each next_state
            next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)
            # The future reward is zero if the episode is done (terminal state)
            target_q_values = rewards + (config.GAMMA * next_q_values * (1 - dones))
        # 3. Compute the loss between current and target Q-values
        loss = self.loss_fn(current_q_values, target_q_values)
        # 4. Perform backpropagation
        self.optimizer.zero_grad()
        loss.backward()
        # Clip gradients to prevent them from exploding, a common practice in RL
        for param in self.policy_net.parameters():
            param.grad.data.clamp_(-1, 1)
        self.optimizer.step()
    def update_target_network(self):
        """
        Updates the target network's weights by copying them from the policy network.
        This is a "hard" update, performed periodically.
        """
        self.target_net.load_state_dict(self.policy_net.state_dict())
    def decay_epsilon(self):
        """
        Decays the epsilon value according to the configured decay rate.
        This reduces exploration over time as the agent becomes more confident.
        """
        self.epsilon = max(config.EPSILON_END, config.EPSILON_DECAY * self.epsilon)
    def save_model(self, episode: int):
        """
        Saves the policy network's weights to a file.
        Args:
            episode (int): The current episode number, used in the filename.
        """
        if not os.path.exists(config.MODEL_PATH):
            os.makedirs(config.MODEL_PATH)
        filename = f"{self.agent_id}_episode_{episode}.pth"
        path = os.path.join(config.MODEL_PATH, filename)
        torch.save(self.policy_net.state_dict(), path)
        print(f"Model for {self.agent_id} saved to {path}")
    def load_model(self, path: str):
        """
        Loads the policy network's weights from a file into both networks.
        Args:
            path (str): The file path to the saved model weights.
        """
        if os.path.exists(path):
            try:
                self.policy_net.load_state_dict(torch.load(path, map_location=device))
                self.target_net.load_state_dict(self.policy_net.state_dict())
                self.policy_net.train()
                self.target_net.eval()
                print(f"Model for {self.agent_id} loaded from {path}")
            except Exception as e:
                print(f"Error loading model from {path}. Starting fresh. Error: {e}")
        else:
            print(f"No model found at {path}, starting from scratch.")
================================================
FILE: ball.py
================================================
import pygame
import math
import numpy as np
import config
class Ball:
    """
    Represents the ball in the game, handling its physics, movement,
    and collision interactions with walls and paddles.
    """
    def __init__(self):
        """
        Initializes the ball object.
        Sets its initial position, size, and velocity. The initial serving
        direction is chosen randomly.
        """
        self.radius = config.BALL_RADIUS
        # Use float position for accurate physics
        self.x = config.SCREEN_WIDTH / 2
        self.y = config.SCREEN_HEIGHT / 2
        # The ball's position is managed by a Pygame Rect for easier collision detection
        # and drawing. The internal position is float-based for physics accuracy.
        self.rect = pygame.Rect(
            self.x - self.radius,
            self.y - self.radius,
            self.radius * 2,
            self.radius * 2
        )
        self.vx = 0.0
        self.vy = 0.0
        self.speed = 0.0
        # Start the first serve randomly
        self.reset(scored_left=np.random.choice([True, False]))
    def move(self, dt=1.0):
        """Updates the ball's position based on its current velocity."""
        self.x += self.vx * dt
        self.y += self.vy * dt
        # Update the rect position for collision detection
        self.rect.centerx = int(self.x)
        self.rect.centery = int(self.y)
    def handle_wall_collision(self):
        """
        Checks for and handles collisions with the top and bottom walls
        by reversing the vertical velocity.
        """
        if self.rect.top <= 0 or self.rect.bottom >= config.SCREEN_HEIGHT:
            self.vy *= -1
            # Clamp position to prevent the ball from getting stuck in a wall
            if self.rect.top < 0:
                self.rect.top = 0
                self.y = self.rect.centery
            if self.rect.bottom > config.SCREEN_HEIGHT:
                self.rect.bottom = config.SCREEN_HEIGHT
                self.y = self.rect.centery
    def handle_paddle_collision(self, paddle):
        """
        Handles collision with a paddle, calculating the new velocity based on
        the paddle's state (swinging or not).
        Args:
            paddle (Paddle): The paddle object the ball has collided with.
        """
        # This check prevents a "double hit" bug where the ball, after reversing
        # direction, is still inside the paddle and collides again. We only
        # process a collision if the ball is moving towards the paddle.
        is_moving_towards_paddle = (paddle.side == 'left' and self.vx < 0) or \
                                   (paddle.side == 'right' and self.vx > 0)
        if not is_moving_towards_paddle:
            return False
        if paddle.is_swinging:
            self._handle_swing_collision(paddle)
        else:
            self._handle_normal_collision(paddle)
        # After collision, push the ball out of the paddle's rect to prevent sticking.
        if paddle.side == 'left':
            self.rect.left = paddle.rect.right
            self.x = self.rect.centerx
        else: # right paddle
            self.rect.right = paddle.rect.left
            self.x = self.rect.centerx
        
        return True
    def increase_speed(self):
        """
        Gradually increases the x velocity to prevent the ball from getting stuck 
        bouncing vertically. Only applies a small boost when x velocity is too low
        relative to y velocity.
        """
        abs_vx = abs(self.vx)
        abs_vy = abs(self.vy)
        
        # Check if ball is moving too vertically
        is_too_vertical = (abs_vx < config.BALL_MIN_X_VELOCITY) or \
                         (abs_vx > 0 and abs_vy / abs_vx > config.BALL_VERTICAL_RATIO_THRESHOLD)
        
        if is_too_vertical:
            # Apply a small, constant boost in the current x direction
            if self.vx > 0:
                self.vx += config.BALL_X_BOOST_PER_FRAME
            elif self.vx < 0:
                self.vx -= config.BALL_X_BOOST_PER_FRAME
            else:
                # If vx is exactly 0, give it a small random direction
                import numpy as np
                self.vx = config.BALL_X_BOOST_PER_FRAME * (1 if np.random.random() > 0.5 else -1)
    def _handle_normal_collision(self, paddle):
        """
        Handles a standard, non-swinging collision, typical of classic Pong.
        The bounce angle is determined by where the ball hits the paddle.
        Args:
            paddle (Paddle): The paddle involved in the collision.
        """
        relative_y = self.rect.centery - paddle.rect.centery
        # Normalize the impact point from -1 (top) to 1 (bottom)
        normalized_y = relative_y / (paddle.height / 2)
        bounce_angle_deg = normalized_y * config.MAX_BOUNCE_ANGLE
        bounce_angle_rad = math.radians(bounce_angle_deg)
        direction = 1 if paddle.side == 'left' else -1
        # self.speed = config.BALL_SPEED_INITIAL
        self.vx = direction * self.speed * math.cos(bounce_angle_rad)
        self.vy = self.speed * math.sin(bounce_angle_rad)
    def _handle_swing_collision(self, paddle):
        """
        Handles a collision when the paddle is swinging, creating a pinball-like effect.
        The ball's exit velocity is calculated by reflecting its incoming velocity
        vector off the paddle's angled surface normal.
        Args:
            paddle (Paddle): The swinging paddle involved in the collision.
        """
        # Get paddle's surface orientation from the paddle object.
        paddle_surface_angle_rad = math.radians(paddle.get_surface_angle())
        # The normal vector is perpendicular to the paddle's surface.
        # For a surface at angle theta, its normal is at angle theta - 90 degrees.
        normal_angle_rad = paddle_surface_angle_rad - math.pi / 2
        # Create the normal vector using Pygame's Vector2 for reflection math
        normal_vector = pygame.math.Vector2(
            math.cos(normal_angle_rad),
            math.sin(normal_angle_rad)
        )
        incoming_velocity = pygame.math.Vector2(self.vx, self.vy)
        # Reflect the velocity vector across the surface normal
        reflected_velocity = incoming_velocity.reflect(normal_vector)
        # Apply the increased speed from the swing
        self.speed = config.BALL_SPEED_SWING
        if reflected_velocity.length() > 0:
            reflected_velocity.scale_to_length(self.speed)
        self.vx = reflected_velocity.x
        self.vy = reflected_velocity.y
    def check_score(self):
        """
        Checks if the ball has gone past a paddle, resulting in a score.
        Returns:
            str or None: 'right' if the right player scored (ball passed left side),
                         'left' if the left player scored (ball passed right side),
                         or None if no one has scored.
        """
        if self.rect.right < 0:
            return 'right'  # Right player scored
        if self.rect.left > config.SCREEN_WIDTH:
            return 'left'   # Left player scored
        return None
    def reset(self, scored_left):
        """
        Resets the ball to the center of the screen and sets a new velocity.
        The ball is served towards the player who just lost the point.
        Args:
            scored_left (bool): True if the left player scored, False otherwise.
        """
        self.x = config.SCREEN_WIDTH / 2
        self.y = config.SCREEN_HEIGHT / 2
        self.rect.center = (int(self.x), int(self.y))
        self.speed = config.BALL_SPEED_INITIAL
        # Ball moves towards the loser.
        # If left scored, ball goes right (positive direction).
        # If right scored, ball goes left (negative direction).
        direction = 1 if scored_left else -1
        
        # Serve at a random angle up to 45 degrees up or down
        angle = np.random.uniform(-math.pi / 4, math.pi / 4)
        self.vx = direction * self.speed * math.cos(angle)
        self.vy = self.speed * math.sin(angle)
    def draw(self, screen):
        """
        Draws the ball on the game screen.
        Args:
            screen (pygame.Surface): The Pygame surface to draw on.
        """
        pygame.draw.ellipse(screen, config.BALL_COLOR, self.rect)

================================================
FILE: config.py
================================================
"""
config.py
This file contains all game constants, hyperparameters, and configuration settings
for the Deep Q-Learning Pong game. Centralizing these parameters makes them
easy to adjust and tune.
"""
# ---------------------------- #
# Game & Display Settings
# ---------------------------- #
SCREEN_WIDTH = 800
SCREEN_HEIGHT = 608
WIN_WIDTH = SCREEN_WIDTH  # Alias for compatibilityWIN_HEIGHT = SCREEN_HEIGHT  # Alias for compatibility
GAME_CAPTION = "RL Flipper Pong"
FPS =60  # Frames per second for rendering
WINNING_SCORE = 4  # Score needed to win a game
# ---------------------------- #
# Colors
# ---------------------------- #
BLACK = (0, 0, 0)
WHITE = (255, 255, 255)
GRAY = (128, 128, 128)
RED = (255, 0, 0)
GREEN = (0, 255, 0)
BLUE = (0, 0, 255)
PADDLE_COLOR = BLUE
BALL_COLOR = RED
# ---------------------------- #
# Physics Constants
# ---------------------------- #
# Defines the fixed time interval used for each physics update (e.g., movement calculations).
# By synchronizing this with FPS (e.g., 1.0 / 60.0 for 60 FPS), we ensure that the game's
# physics are consistent and deterministic, which is crucial for stable RL training.
# This decouples game speed from rendering speed; the game will run at the same speed
# regardless of whether the visual frame rate is high or low. A higher physics update
# rate (smaller timestep) prevents "tunneling" where fast-moving objects can pass
# through thin obstacles.
FIXED_TIMESTEP = 1.0 / 70.0  # Timestep for physics updates, independent of FPS
# ---------------------------- #
# Paddle Constants
# ---------------------------- #
PADDLE_WIDTH = 15
PADDLE_HEIGHT = 100
PADDLE_SPEED = 10.0
PADDLE_WALL_OFFSET = 20  # Distance from the side walls
# Flipper/Swing Mechanics
PADDLE_MAX_SWING_ANGLE = 42  # Max rotation in degrees
PADDLE_SWING_DURATION = 0.2  # Duration of the swing animation in seconds
# ---------------------------- #
# Ball Constants
# ---------------------------- #
BALL_RADIUS = 10
BALL_INITIAL_SPEED_X = 300.0
BALL_INITIAL_SPEED_Y = 200.0
BALL_MAX_SPEED = 900.0  # Maximum speed magnitude for the ball
BALL_SPEED_INCREASE_FACTOR = 1.51  # Factor by which speed increases on paddle hit
BALL_SPEED_INITIAL = 450.0  # Initial ball speed (fast enough to reach paddles)
BALL_SPEED_SWING = 700.0   # Ball speed after swing hit
MAX_BOUNCE_ANGLE = 44     # Maximum bounce angle in degrees
# Anti-stall parameters (to prevent vertical bouncing)
BALL_MIN_X_VELOCITY = 60.0  # Minimum acceptable x velocity magnitude
BALL_VERTICAL_RATIO_THRESHOLD = 7.0  # If |vy| / |vx| > this, ball is too vertical
BALL_X_BOOST_PER_FRAME = 1.0  # Small constant boost applied per frame when ball is too vertical
# ---------------------------- #
# Reinforcement Learning Hyperparameters
# ---------------------------- #
# State & Action Space
# State: [ball_x, ball_y, ball_vx, ball_vy, player1_y, player2_y, player1_swing_timer, player2_swing_timer]
STATE_SIZE = 8
# Actions: [STAY, UP, DOWN, SWING]
ACTION_SIZE = 4
# DQN Agent Settings
MEMORY_SIZE = 100000        # Replay buffer size
BATCH_SIZE = 512           # Minibatch size for training
GAMMA = 0.99               # Discount factor for future rewards
LEARNING_RATE = 0.0001     # Learning rate for the Adam optimizer
# Epsilon-Greedy Policy for Exploration
EPSILON_START = 0.39      # Initial exploration rate
EPSILON_END = 0.01         # Minimum exploration rate
EPSILON_DECAY = 0.997        # Decay rate for epsilon per episode
# Target Network Update
TARGET_UPDATE_FREQUENCY = 10 # Update target network every N episodes
# ---------------------------- #
# Reward Structure
# ---------------------------- #
REWARD_WIN = 6.0          # Reward for scoring a point
REWARD_LOSE = -6.0        # Reward for conceding a point
REWARD_HIT = 0.2          # Small positive reward for hitting the ball
REWARD_SWING = -0.01      # Small penalty for swinging paddle (discourages unnecessary swings)
REWARD_GAME_WIN = 9.0     # Large reward for winning the entire game (reaching WINNING_SCORE)
REWARD_GAME_LOSE = -9.0   # Large penalty for losing the entire game
# ---------------------------- #
# Training & Model Settings
# ---------------------------- #
NUM_EPISODES = 12000       # Total number of games to play for training
MAX_STEPS_PER_EPISODE = 4200  # Maximum steps per episode (60 seconds at 30 FPS)
DISPLAY_EVERY = 20         # Render every Nth game
SAVE_MODEL_EVERY = 100    # Save trained models every N episodes
MODEL_PATH = "models/"    # Directory to save/load models
LOAD_MODEL = True        # Set to True to load a pre-trained model
MODEL_TO_LOAD_P1 = "models/agent1_episode_600.pth" # Example path
MODEL_TO_LOAD_P2 = "models/agent2_episode_600.pth" # Example path
# V-trace / Actor-Critic Settings
# ENTROPY_BETA: Coefficient for the entropy bonus in the policy loss.
# The entropy bonus encourages exploration by penalizing the policy for being too
# certain. A higher value promotes more random actions, which can help the agent
# discover better strategies, especially early in training.
# - Increasing this value leads to more exploration but can prevent the policy
#   from converging to an optimal solution if too high.
# - Decreasing this value leads to more exploitation, faster convergence, but
#   risks getting stuck in a local optimum.
ENTROPY_BETA = 0.02
# VTRACE_RHO_CLIP: The clipping threshold for the rho importance sampling ratio,
# used for the V-trace value target calculation. This corrects for the difference
# between the target policy and the behavior policy (off-policy correction).
# Clipping prevents the value updates from becoming too large and unstable.
# - Increasing this value allows for more aggressive off-policy corrections,
#   which can speed up learning but may increase variance and instability.
# - Decreasing this value makes the value updates more conservative and stable,
#   but can slow down learning. A value of 1.0 is a common default.
VTRACE_RHO_CLIP = 1.0
# VTRACE_C_CLIP: The clipping threshold for the c importance sampling ratio,
# used for the V-trace policy gradient calculation. This also corrects for the
# off-policy data. Clipping stabilizes the policy gradient updates.
# - Increasing this value allows the policy to change more drastically based on
#   past experiences, which can be faster but riskier.
# - Decreasing this value makes policy updates smaller and more stable, but
#   potentially slower. A value of 1.0 is a common default.
VTRACE_C_CLIP = 1.0

================================================
FILE: game.py
================================================
import pygame
import numpy as np
import math
# Internal project imports
import config
from ball import Ball
from paddle import Paddle
class PongGame:
    """
    Manages the overall game state, physics, rendering, and agent interactions.
    This class orchestrates the game loop, including handling player actions,
    updating the physics of the ball and paddles with a fixed timestep,
    detecting collisions, managing the scoring system, and providing a
    normalized state representation for the reinforcement learning agents.
    """
    def __init__(self, render_mode: bool = False):
        """
        Initializes the Pong game environment.
        Args:
            render_mode (bool): If True, initializes Pygame for graphical display.
                                Set to False for faster, non-displayed training.
        """
        self.render_mode = render_mode
        self.screen = None
        self.clock = None
        self.font = None
        if self.render_mode:
            pygame.init()
            pygame.display.set_caption(config.GAME_CAPTION)
            self.screen = pygame.display.set_mode((config.SCREEN_WIDTH, config.SCREEN_HEIGHT))
            self.clock = pygame.time.Clock()
            self.font = pygame.font.Font(None, 36)
        # Create game objects
        self.paddle1 = Paddle(config.PADDLE_WALL_OFFSET, 'left')
        self.paddle2 = Paddle(config.SCREEN_WIDTH - config.PADDLE_WIDTH - config.PADDLE_WALL_OFFSET, 'right')
        self.ball = Ball()
        # Game state variables
        self.score1 = 0
        self.score2 = 0
        self.time_accumulator = 0.0
    def reset(self) -> np.ndarray:
        """
        Resets the game to its initial state for a new episode.
        This involves resetting scores, and the positions and states of the
        paddles and ball.
        Returns:
            np.ndarray: The initial state of the game environment.
        """
        self.score1 = 0
        self.score2 = 0
        self.paddle1.reset()
        self.paddle2.reset()
        # Reset ball, ensuring it serves towards a random side initially
        self.ball.reset(scored_left=np.random.choice([True, False]))
        self.time_accumulator = 0.0
        return self._get_state()
    def step(self, action1: int, action2: int, dt: float) -> tuple[np.ndarray, float, float, bool]:
        """
        Executes one time step of the game.
        Args:
            action1 (int): The action chosen by agent 1.
            action2 (int): The action chosen by agent 2.
            dt (float): The delta time since the last frame, in seconds.
        Returns:
            tuple: A tuple containing:
                - next_state (np.ndarray): The state of the game after the step.
                - reward1 (float): The reward for agent 1.
                - reward2 (float): The reward for agent 2.
                - done (bool): True if the game has ended, False otherwise.
        """
        self._apply_actions(action1, action2)
        
        # Update physics using a fixed timestep for consistency
        self.time_accumulator += dt
        while self.time_accumulator >= config.FIXED_TIMESTEP:
            self._update_physics(config.FIXED_TIMESTEP)
            self.time_accumulator -= config.FIXED_TIMESTEP
        # Check for scoring and collisions, and calculate rewards
        hit_reward1, hit_reward2 = self._handle_collisions()
        score_reward1, score_reward2 = self._handle_scoring()
        
        reward1 = hit_reward1 + score_reward1
        reward2 = hit_reward2 + score_reward2
        # Check if the game is over
        done = (self.score1 >= config.WINNING_SCORE or
                self.score2 >= config.WINNING_SCORE)
        next_state = self._get_state()
        return next_state, reward1, reward2, done
    def _apply_actions(self, action1: int, action2: int):
        """Maps integer actions to paddle movements and swings."""
        # Action mapping: 0: STAY, 1: UP, 2: DOWN, 3: SWING
        # Player 1 (Left Paddle)
        if action1 == 1:
            self.paddle1.move(-1)  # -1 for UP
        elif action1 == 2:
            self.paddle1.move(1)   # 1 for DOWN
        elif action1 == 3:
            self.paddle1.swing()
        # Player 2 (Right Paddle)
        if action2 == 1:
            self.paddle2.move(-1)
        elif action2 == 2:
            self.paddle2.move(1)
        elif action2 == 3:
            self.paddle2.swing()
    def _update_physics(self, fixed_dt: float):
        """Updates all game objects by one fixed time step."""
        self.paddle1.update(fixed_dt)
        self.paddle2.update(fixed_dt)
        self.ball.move(fixed_dt)
        self.ball.handle_wall_collision()
        # Increase ball speed each step until a point is scored
        self.ball.increase_speed()
    def _handle_collisions(self) -> tuple[float, float]:
        """
        Checks for and handles ball-paddle collisions.
        Returns:
            tuple[float, float]: Rewards for agent 1 and agent 2 for hitting the ball.
        """
        reward1, reward2 = 0.0, 0.0
        
        # AABB broad-phase collision check
        if self.ball.rect.colliderect(self.paddle1.rect):
            hit_occured = self.ball.handle_paddle_collision(self.paddle1)
            if hit_occured:
                reward1 = config.REWARD_HIT
        
        if self.ball.rect.colliderect(self.paddle2.rect):
            hit_occured = self.ball.handle_paddle_collision(self.paddle2)
            if hit_occured:
                reward2 = config.REWARD_HIT
                
        return reward1, reward2
    def _handle_scoring(self) -> tuple[float, float]:
        """
        Checks if a player has scored, updates scores, and resets the ball.
        Returns:
            tuple[float, float]: Rewards for agent 1 and agent 2 from scoring.
        """
        reward1, reward2 = 0.0, 0.0
        
        scorer = self.ball.check_score()
        if scorer:
            if scorer == 'left':  # Left player (Agent 1) scored
                self.score1 += 1
                reward1 = config.REWARD_WIN
                reward2 = config.REWARD_LOSE
                self.ball.reset(scored_left=False)  # Serve to the loser (right)
            elif scorer == 'right':  # Right player (Agent 2) scored
                self.score2 += 1
                reward1 = config.REWARD_LOSE
                reward2 = config.REWARD_WIN
                self.ball.reset(scored_left=True)   # Serve to the loser (left)
        return reward1, reward2
    def _get_state(self) -> np.ndarray:
        """
        Generates the normalized state vector for the RL agents.
        The state is absolute (world-centric), not relative to each player.
        This provides a complete picture of the environment. Normalization
        is crucial for stable neural network training.
        State Vector Composition:
        [ball_x, ball_y, ball_vx, ball_vy, paddle1_y, paddle2_y, paddle1_swing_timer, paddle2_swing_timer]
        Returns:
            np.ndarray: A numpy array representing the normalized game state.
        """
        # Normalize ball position (0 to 1)
        ball_x_norm = self.ball.rect.centerx / config.SCREEN_WIDTH
        ball_y_norm = self.ball.rect.centery / config.SCREEN_HEIGHT
        # Normalize ball velocity (-1 to 1)
        ball_vx_norm = np.clip(self.ball.vx / config.BALL_MAX_SPEED, -1, 1)
        ball_vy_norm = np.clip(self.ball.vy / config.BALL_MAX_SPEED, -1, 1)
        # Normalize paddle positions (0 to 1)
        p1_y_norm = self.paddle1.rect.centery / config.SCREEN_HEIGHT
        p2_y_norm = self.paddle2.rect.centery / config.SCREEN_HEIGHT
        # Normalize swing timers (0 to 1, indicates swing progress)
        p1_swing_norm = self.paddle1.swing_timer / config.PADDLE_SWING_DURATION
        p2_swing_norm = self.paddle2.swing_timer / config.PADDLE_SWING_DURATION
        state = np.array([
            ball_x_norm, ball_y_norm, ball_vx_norm, ball_vy_norm,
            p1_y_norm, p2_y_norm, p1_swing_norm, p2_swing_norm
        ], dtype=np.float32)
        return state
    def render(self, game_num: int):
        """
        Draws the current game state to the screen.
        Args:
            game_num (int): The current episode number to display.
        """
        if not self.render_mode:
            return
        self.screen.fill(config.BLACK)
        # Draw paddles and ball
        self.paddle1.draw(self.screen)
        self.paddle2.draw(self.screen)
        self.ball.draw(self.screen)
        # Draw a center line
        pygame.draw.aaline(self.screen, config.GRAY,
                           (config.SCREEN_WIDTH / 2, 0),
                           (config.SCREEN_WIDTH / 2, config.SCREEN_HEIGHT))
        # Render scores
        score_text = self.font.render(f"{self.score1}  -  {self.score2}", True, config.WHITE)
        self.screen.blit(score_text, (config.SCREEN_WIDTH / 2 - score_text.get_width() / 2, 10))
        
        # Render game number
        game_text = self.font.render(f"Game: {game_num}", True, config.GRAY)
        self.screen.blit(game_text, (10, 10))
        pygame.display.flip()
        self.clock.tick(config.FPS)
    def close(self):
        """Cleans up and closes the Pygame window."""
        if self.render_mode:
            pygame.quit()

================================================
FILE: main_visual.py
================================================
import pygame
import sys
import os
import time
import numpy as np
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
import imageio
import pygame
import numpy as np
import os
import config
from ball import Ball
from paddle import Paddle
from agent import Agent
from vtrace_agent import VTraceAgent
class Recorder:
    def __init__(self, path, fps=30):
        self.writer = imageio.get_writer(path, fps=(1/config.FIXED_TIMESTEP))
    def capture(self, screen):
        arr = pygame.surfarray.array3d(screen)
        frame = np.transpose(arr, (1, 0, 2))  # Convert (W, H, C) -> (H, W, C)
        self.writer.append_data(frame)
    def close(self):
        self.writer.close()
def create_episode_summary_table(episode, step_count, hits_p1, hits_p2, swings_p1, swings_p2, 
                                total_reward_p1, total_reward_p2, total_wins_p1, total_wins_p2, 
                                cumulative_total_avg_reward, episode_result=None):
    """
    Create and return an episode summary table.
    
    Args:
        episode_result: Optional string to indicate special episode endings (e.g., "TIMEOUT")
    """
    total_hits = hits_p1 + hits_p2
    total_swings = swings_p1 + swings_p2
    summary_table = Table(show_header=False, box=None, padding=(0, 1))
    summary_table.add_column("", style="cyan")
    summary_table.add_column("", style="white")
    # Add episode result row if specified
    if episode_result:
        if episode_result == "TIMEOUT":
            summary_table.add_row("‚è∞ Episode Result", "[red]TIMEOUT - No Winner[/red]")
        else:
            summary_table.add_row("üèÜ Episode Result", episode_result)
    summary_table.add_row("ü•ä Swing %", f"{total_swings / (step_count * 2) * 100:.3f}%")
    summary_table.add_row("üèì Hits", f"{total_hits}")
    summary_table.add_row("üèì Swings", f"{total_swings}")
    summary_table.add_row("üìà P1 Reward", f"{total_reward_p1:.3f}")
    summary_table.add_row(
        "üìà P2 Reward", 
        f"{total_reward_p2:.3f}"
    )
    summary_table.add_row("üèÜ Total Wins", f"P1: {total_wins_p1} | P2: {total_wins_p2}")
    return summary_table
def main():
    """
    Visual version with forced longer episodes and debug info.
    """
    # Initialize rich console
    console = Console()
    pygame.init()
    screen = pygame.display.set_mode((config.SCREEN_WIDTH, config.SCREEN_HEIGHT))
    pygame.display.set_caption(config.GAME_CAPTION + " - Visual Training")
    clock = pygame.time.Clock()
    font = pygame.font.Font(None, 32)
    small_font = pygame.font.Font(None, 20)
    console.print(Panel.fit("ü§ñ Creating RL agents...", style="bold blue"))
    agent1 = Agent(agent_id='agent1')
    agent2 = VTraceAgent(agent_id='agent2')
    # Load pre-trained models if enabled
    if config.LOAD_MODEL:
        console.print(Panel.fit("üì• Loading pre-trained models...", style="bold cyan"))
        # Load Player 1 model
        if os.path.exists(config.MODEL_TO_LOAD_P1):
            agent1.load_model(config.MODEL_TO_LOAD_P1)
            console.print(f"[green]‚úÖ Player 1 model loaded from:[/green] {config.MODEL_TO_LOAD_P1}")
        else:
            console.print(f"[red]‚ùå Player 1 model not found:[/red] {config.MODEL_TO_LOAD_P1}")
            console.print("[yellow]‚ö†Ô∏è  Player 1 will start training from scratch[/yellow]")
        # Load Player 2 model
        if os.path.exists(config.MODEL_TO_LOAD_P2):
            agent2.load_model(config.MODEL_TO_LOAD_P2)
            console.print(f"[green]‚úÖ Player 2 model loaded from:[/green] {config.MODEL_TO_LOAD_P2}")
        else:
            console.print(f"[red]‚ùå Player 2 model not found:[/red] {config.MODEL_TO_LOAD_P2}")
            console.print("[yellow]‚ö†Ô∏è  Player 2 will start training from scratch[/yellow]")
        console.print()  # Add spacing
    if not os.path.exists(config.MODEL_PATH):
        os.makedirs(config.MODEL_PATH)
    total_wins_p1 = 0
    total_wins_p2 = 0
    running = True
    paused = False
    # Create training info table
    training_table = Table(title="üèì Training Configuration")
    training_table.add_column("Parameter", style="cyan")
    training_table.add_column("Value", style="magenta")
    training_table.add_row("Total Episodes", str(config.NUM_EPISODES))
    training_table.add_row("Display Every", f"{config.DISPLAY_EVERY} episodes")
    training_table.add_row("Winning Score", str(config.WINNING_SCORE))
    training_table.add_row("Ball Speed", f"{config.BALL_SPEED_INITIAL}")
    training_table.add_row("Load Models", "Yes" if config.LOAD_MODEL else "No")
    training_table.add_row("Save Every", f"{config.SAVE_MODEL_EVERY} episodes")
    console.print(training_table)
    console.print("\n[bold yellow]Controls:[/bold yellow] [green]ESC[/green] to quit, [green]P[/green] to pause/unpause, [green]SPACE[/green] to skip episode\n")
    for episode in range(1, config.NUM_EPISODES + 1):
        if not running:
            break
        # Decide whether to render this episode
        should_render = (episode % config.DISPLAY_EVERY == 0)
        if should_render:
            console.print(f"[bold green]üì∫ Episode {episode} starting (VISUAL)...[/bold green]")
            recorder = Recorder(f"videos/game_{episode:04}.mp4")
        else:
            console.print(f"[dim]Episode {episode} starting...[/dim]")
        # Create game objects
        paddle1 = Paddle(config.PADDLE_WALL_OFFSET, 'left')
        paddle2 = Paddle(config.SCREEN_WIDTH - config.PADDLE_WIDTH - config.PADDLE_WALL_OFFSET, 'right')
        ball = Ball()
        score1 = 0
        score2 = 0
        episode_over = False
        step_count = 0
        episode_start_time = time.time()
        # Track hits, swings, and rewards for this episode
        hits_p1 = 0
        hits_p2 = 0
        swings_p1 = 0
        swings_p2 = 0
        total_reward_p1 = 0.0
        total_reward_p2 = 0.0
        cumulative_total_avg_reward = 0.0  # For calculating cumulative average reward
        while not episode_over and running and step_count < config.MAX_STEPS_PER_EPISODE:
            step_count += 1
            # Handle pygame events only when rendering
            if should_render:
                for event in pygame.event.get():
                    if event.type == pygame.QUIT:
                        running = False
                        episode_over = True
                    elif event.type == pygame.KEYDOWN:
                        if event.key == pygame.K_ESCAPE:
                            running = False
                            episode_over = True
                        elif event.key == pygame.K_p:
                            paused = not paused
                            status = "[red]‚è∏Ô∏è  Paused[/red]" if paused else "[green]‚ñ∂Ô∏è  Resumed[/green]"
                            console.print(status)
                        elif event.key == pygame.K_SPACE:
                            episode_over = True
            if not running:
                break
            if paused and should_render:
                # Render pause state
                screen.fill(config.BLACK)
                pause_text = font.render("PAUSED - Press P to continue", True, config.WHITE)
                pause_rect = pause_text.get_rect(center=(config.SCREEN_WIDTH // 2, config.SCREEN_HEIGHT // 2))
                screen.blit(pause_text, pause_rect)
                pygame.display.flip()
                clock.tick(10)
                continue
            # Get normalized game state
            ball_x_norm = ball.rect.centerx / config.SCREEN_WIDTH
            ball_y_norm = ball.rect.centery / config.SCREEN_HEIGHT
            ball_vx_norm = np.clip(ball.vx / config.BALL_MAX_SPEED, -1, 1)
            ball_vy_norm = np.clip(ball.vy / config.BALL_MAX_SPEED, -1, 1)
            p1_y_norm = paddle1.rect.centery / config.SCREEN_HEIGHT
            p2_y_norm = paddle2.rect.centery / config.SCREEN_HEIGHT
            p1_swing_norm = paddle1.swing_timer / config.PADDLE_SWING_DURATION if paddle1.swing_timer > 0 else 0
            p2_swing_norm = paddle2.swing_timer / config.PADDLE_SWING_DURATION if paddle2.swing_timer > 0 else 0
            state = np.array([
                ball_x_norm, ball_y_norm, ball_vx_norm, ball_vy_norm,
                p1_y_norm, p2_y_norm, p1_swing_norm, p2_swing_norm
            ], dtype=np.float32)
            # AI agents choose actions
            action1 = agent1.choose_action(state)
            action2 = agent2.choose_action(state)
            # Apply actions to paddles
            if action1 == 1:
                paddle1.move(-1)  # UP
            elif action1 == 2:
                paddle1.move(1)   # DOWN
            elif action1 == 3:
                paddle1.swing()   # SWING
            if action2 == 1:
                paddle2.move(-1)  # UP
            elif action2 == 2:
                paddle2.move(1)   # DOWN
            elif action2 == 3:
                paddle2.swing()   # SWING
            # Update physics
            dt = config.FIXED_TIMESTEP  # Use fixed timestep for physics updates
            paddle1.update(dt)
            paddle2.update(dt)
            ball.move(dt)
            ball.handle_wall_collision()
            ball.increase_speed()  # Apply anti-stall mechanism
            # Handle ball-paddle collisions and apply swing penalties
            reward1 = 0.0
            reward2 = 0.0
            # Apply swing penalties and count swings
            if action1 == 3:
                reward1 += config.REWARD_SWING
                swings_p1 += 1
            if action2 == 3:
                reward2 += config.REWARD_SWING
                swings_p2 += 1
            if ball.rect.colliderect(paddle1.rect):
                if ball.handle_paddle_collision(paddle1):
                    reward1 += config.REWARD_HIT
                    hits_p1 += 1
            if ball.rect.colliderect(paddle2.rect):
                if ball.handle_paddle_collision(paddle2):
                    reward2 += config.REWARD_HIT  
                    hits_p2 += 1
            # Check for scoring
            scorer = ball.check_score()
            round_done = False
            if scorer:
                if should_render:
                    # pause for 1 second to show the score
                    pygame.time.delay(100)
                if scorer == 'left':
                    score1 += 1
                    reward1 += config.REWARD_WIN
                    reward2 += config.REWARD_LOSE
                else:
                    score2 += 1
                    reward1 += config.REWARD_LOSE
                    reward2 += config.REWARD_WIN
                ball.reset(scored_left=(scorer == 'right'))
                # End episode immediately if someone reached winning score
                if score1 >= config.WINNING_SCORE or score2 >= config.WINNING_SCORE:
                    episode_over = True
                    round_done = True
                    # Add game win/lose rewards
                    if score1 >= config.WINNING_SCORE:
                        reward1 += config.REWARD_GAME_WIN
                        reward2 += config.REWARD_GAME_LOSE
                        total_wins_p1 += 1
                    else:
                        reward1 += config.REWARD_GAME_LOSE
                        reward2 += config.REWARD_GAME_WIN
                        total_wins_p2 += 1
            # Accumulate total rewards for this episode
            total_reward_p1 += reward1
            total_reward_p2 += reward2
            # Get next state
            next_state = np.array([
                ball.rect.centerx / config.SCREEN_WIDTH,
                ball.rect.centery / config.SCREEN_HEIGHT,
                np.clip(ball.vx / config.BALL_MAX_SPEED, -1, 1),
                np.clip(ball.vy / config.BALL_MAX_SPEED, -1, 1),
                paddle1.rect.centery / config.SCREEN_HEIGHT,
                paddle2.rect.centery / config.SCREEN_HEIGHT,
                paddle1.swing_timer / config.PADDLE_SWING_DURATION if paddle1.swing_timer > 0 else 0,
                paddle2.swing_timer / config.PADDLE_SWING_DURATION if paddle2.swing_timer > 0 else 0
            ], dtype=np.float32)
            # Store experiences and train
            agent1.remember(state, action1, reward1, next_state, round_done)
            agent2.remember(state, action2, reward2, next_state, round_done)
            agent1.learn()
            agent2.learn()
            # Render the game only if this episode should be displayed
            if should_render:
                screen.fill(config.BLACK)
                # Draw game elements
                paddle1.draw(screen)
                paddle2.draw(screen)
                ball.draw(screen)
                # Draw center line
                pygame.draw.aaline(screen, config.GRAY,
                                   (config.SCREEN_WIDTH // 2, 0),
                                   (config.SCREEN_WIDTH // 2, config.SCREEN_HEIGHT))
                # Draw scores (large)
                score_text = font.render(f"{score1}  -  {score2}", True, config.WHITE)
                screen.blit(score_text, (config.SCREEN_WIDTH // 2 - score_text.get_width() // 2, 30))
                # Draw info panel
                info_y = 10
                episode_text = small_font.render(f"Episode: {episode}/{config.NUM_EPISODES}", True, config.WHITE)
                screen.blit(episode_text, (10, info_y))
                info_y += 22
                step_text = small_font.render(f"Step: {step_count}/{config.MAX_STEPS_PER_EPISODE}", True, config.WHITE)
                screen.blit(step_text, (10, info_y))
                info_y += 22
                # Calculate and display vertical ratio (same logic as in ball.py)
                abs_vx = abs(ball.vx)
                abs_vy = abs(ball.vy)
                vertical_ratio = abs_vy / abs_vx if abs_vx > 0 else float('inf')
                ratio_color = config.RED if vertical_ratio > config.BALL_VERTICAL_RATIO_THRESHOLD else config.WHITE
                if vertical_ratio > config.BALL_VERTICAL_RATIO_THRESHOLD:
                    ratio_text = small_font.render(f"Vertical Ratio: {vertical_ratio:.2f} (threshold: {config.BALL_VERTICAL_RATIO_THRESHOLD})", True, ratio_color)
                    screen.blit(ratio_text, (10, info_y))
                    info_y += 22
                # Show anti-stall status (same logic as in ball.py increase_speed method)
                is_too_vertical = (abs_vx < config.BALL_MIN_X_VELOCITY) or \
                                 (abs_vx > 0 and abs_vy / abs_vx > config.BALL_VERTICAL_RATIO_THRESHOLD)
                stall_status = "ACTIVE" if is_too_vertical else "INACTIVE"
                stall_color = config.GREEN if is_too_vertical else config.GRAY
                if is_too_vertical:
                    stall_text = small_font.render(f"Anti-stall: {stall_status} (min_vx: {config.BALL_MIN_X_VELOCITY})", True, stall_color)
                    screen.blit(stall_text, (10, info_y))
                    info_y += 22
                eps_text = small_font.render(f"Epsilon: {agent1.epsilon:.3f}", True, config.WHITE)
                screen.blit(eps_text, (10, info_y))
                info_y += 20
                wins_text = small_font.render(f"Wins: P1={total_wins_p1}, P2={total_wins_p2}", True, config.WHITE)
                screen.blit(wins_text, (10, info_y))
                pygame.display.flip()
                clock.tick(config.FPS)  # 60 FPS
                if recorder:
                    recorder.capture(screen)
        # Check if episode should end due to timeout
        if step_count >= config.MAX_STEPS_PER_EPISODE:
            episode_over = True
        # Handle episode ending (either by win/loss or timeout)
        if episode_over:
            # Handle timeout scenario
            if step_count >= config.MAX_STEPS_PER_EPISODE and score1 < config.WINNING_SCORE and score2 < config.WINNING_SCORE:
                # Both players get penalty for timeout
                timeout_reward1 = config.REWARD_GAME_LOSE
                timeout_reward2 = config.REWARD_GAME_LOSE
                # Accumulate timeout penalties
                total_reward_p1 += timeout_reward1
                total_reward_p2 += timeout_reward2
                # Create final experience for timeout
                final_next_state = np.array([
                    ball.rect.centerx / config.SCREEN_WIDTH,
                    ball.rect.centery / config.SCREEN_HEIGHT,
                    np.clip(ball.vx / config.BALL_MAX_SPEED, -1, 1),
                    np.clip(ball.vy / config.BALL_MAX_SPEED, -1, 1),
                    paddle1.rect.centery / config.SCREEN_HEIGHT,
                    paddle2.rect.centery / config.SCREEN_HEIGHT,
                    paddle1.swing_timer / config.PADDLE_SWING_DURATION if paddle1.swing_timer > 0 else 0,
                    paddle2.swing_timer / config.PADDLE_SWING_DURATION if paddle2.swing_timer > 0 else 0
                ], dtype=np.float32)
                # Store timeout experiences
                agent1.remember(state, action1, timeout_reward1, final_next_state, True)  # done=True for timeout
                agent2.remember(state, action2, timeout_reward2, final_next_state, True)  # done=True for timeout
                # Learn from timeout experience
                agent1.learn()
                agent2.learn()
                episode_result = "TIMEOUT"
                if should_render:
                    console.print(f"[bold red]‚è∞ Episode {episode} timed out after {step_count} steps - both players penalized[/bold red]")
            else:
                episode_result = None
            # Calculate cumulative average reward for episode summary
            avg_reward_p1 = total_reward_p1 / step_count if step_count > 0 else 0
            avg_reward_p2 = total_reward_p2 / step_count if step_count > 0 else 0
            total_avg_reward = avg_reward_p1 + avg_reward_p2
            cumulative_total_avg_reward = (cumulative_total_avg_reward * (episode - 1) + total_avg_reward) / episode
            if should_render:
                if recorder:
                    recorder.close()
            # Create and print episode summary table for all episode endings
            summary_table = create_episode_summary_table(
                episode, step_count, hits_p1, hits_p2, swings_p1, swings_p2,
                total_reward_p1, total_reward_p2, total_wins_p1, total_wins_p2,
                cumulative_total_avg_reward, episode_result
            )
            console.print(summary_table)
        # Post-episode processing
        agent1.decay_epsilon()
        agent2.decay_epsilon()
        # Save models periodically
        if episode % config.SAVE_MODEL_EVERY == 0:
            agent1.save_model(episode)
            agent2.save_model(episode)
            console.print(f"[bold green]üíæ Episode {episode}: Models saved[/bold green]")
        # Update target networks
        if episode % config.TARGET_UPDATE_FREQUENCY == 0:
            agent1.update_target_network()
            agent2.update_target_network()
            console.print(f"[bold yellow]üîÑ Episode {episode}: Target networks updated[/bold yellow]")
    # Training completion summary
    final_table = Table(title="üéØ Training Completed!")
    final_table.add_column("Player", style="cyan")
    final_table.add_column("Wins", style="magenta")
    final_table.add_column("Win Rate", style="green")
    total_games = total_wins_p1 + total_wins_p2
    p1_rate = (total_wins_p1 / total_games * 100) if total_games > 0 else 0
    p2_rate = (total_wins_p2 / total_games * 100) if total_games > 0 else 0
    final_table.add_row("Player 1", str(total_wins_p1), f"{p1_rate:.1f}%")
    final_table.add_row("Player 2", str(total_wins_p2), f"{p2_rate:.1f}%")
    console.print(final_table)
    console.print("[bold green]üéâ Training session complete![/bold green]")
    pygame.quit()
    sys.exit()
if __name__ == '__main__':
    main()
================================================
FILE: neural_network.py
================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
import config
class DQN(nn.Module):
    """
    Deep Q-Network (DQN) model for the RL agent.
    This is a feed-forward neural network that takes the game state as input
    and outputs the expected Q-value for each possible action. The architecture
    consists of fully connected layers with ReLU activation functions for the
    hidden layers.
    """
    def __init__(self, state_size: int, action_size: int):
        """
        Initializes the neural network layers.
        Args:
            state_size (int): The dimension of the input state space. This corresponds
                              to the number of features in the state vector.
            action_size (int): The number of possible actions the agent can take,
                               which is the output dimension of the network.
        """
        super(DQN, self).__init__()
        # Define the network layers. A simple multi-layer perceptron (MLP).
        # The number of neurons in hidden layers (256, 128) are chosen to provide
        # sufficient capacity for learning the game's dynamics without being
        # excessively large for this problem.
        # Input layer: state_size -> 256 neurons
        self.fc1 = nn.Linear(state_size, 256)
        
        # Hidden layer: 256 -> 128 neurons
        self.fc2 = nn.Linear(256, 128)
        
        # Output layer: 128 -> action_size neurons
        # The output neurons correspond to the Q-values for each action.
        self.fc3 = nn.Linear(128, action_size)
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """
        Defines the forward pass of the network.
        This method takes a state tensor and passes it through the network layers
        to produce Q-values.
        Args:
            state (torch.Tensor): The input tensor representing one or more game states.
                                  Shape should be (batch_size, state_size).
        Returns:
            torch.Tensor: The output tensor containing the Q-values for each action
                          for each state in the batch. Shape: (batch_size, action_size).
        """
        # Pass the input state through the first fully connected layer,
        # followed by a ReLU activation function.
        x = F.relu(self.fc1(state))
        
        # Pass the result through the second hidden layer, also with ReLU activation.
        x = F.relu(self.fc2(x))
        
        # The final layer is the output layer. It has no activation function,
        # as it represents the raw, unbounded Q-values for each action.
        q_values = self.fc3(x)
        
        return q_values
class ActorCriticNet(nn.Module):
    """Simple actor-critic network producing policy logits and state value."""
    def __init__(self, state_size: int, action_size: int):
        super().__init__()
        self.fc1 = nn.Linear(state_size, 256)
        self.fc2 = nn.Linear(256, 128)
        self.policy_head = nn.Linear(128, action_size)
        self.value_head = nn.Linear(128, 1)
    def forward(self, state: torch.Tensor):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        logits = self.policy_head(x)
        value = self.value_head(x)
        return logits, value
================================================
FILE: paddle.py
================================================
import pygame
import math
import numpy as np
# Internal project import
import config
class Paddle:
    """
    Represents a player's paddle with vertical movement and pinball-like swing mechanics.
    The paddle can move up and down along the edge of the screen and perform a
    'swing' action that rotates it toward the center of the screen for a brief
    period. This rotation affects the ball's bounce angle and speed.
    """
    def __init__(self, x: float, side: str):
        """
        Initializes the Paddle object.
        Args:
            x (float): The initial x-coordinate for the paddle's top-left corner.
            side (str): The side of the screen the paddle is on. Must be 'left' or 'right'.
                        This determines the direction of the swing.
        """
        if side not in ['left', 'right']:
            raise ValueError("Side must be 'left' or 'right'")
        self.initial_x = x
        self.initial_y = config.SCREEN_HEIGHT / 2 - config.PADDLE_HEIGHT / 2
        self.side = side
        self.width = config.PADDLE_WIDTH
        self.height = config.PADDLE_HEIGHT
        
        # The un-rotated rectangular area of the paddle
        self.rect = pygame.Rect(self.initial_x, self.initial_y, self.width, self.height)
        
        self.color = config.PADDLE_COLOR
        self.speed = config.PADDLE_SPEED
        # Swing related attributes
        self.angle = 0.0  # Current rotation angle in degrees
        self.angular_velocity = 0.0  # Degrees per second, useful for physics
        self.is_swinging = False
        self.swing_duration = config.PADDLE_SWING_DURATION
        self.max_swing_angle = config.PADDLE_MAX_SWING_ANGLE
        self.swing_timer = 0.0
    def reset(self):
        """Resets the paddle to its initial position and state."""
        self.rect.y = self.initial_y
        self.is_swinging = False
        self.swing_timer = 0.0
        self.angle = 0.0
        self.angular_velocity = 0.0
    def move(self, direction: int):
        """
        Moves the paddle vertically based on a discrete action.
        Args:
            direction (int): The direction to move. 1 for down, -1 for up, 0 for stay.
        """
        if direction not in [-1, 0, 1]:
            return  # Ignore invalid directions
        
        self.rect.y += direction * self.speed
        
        # Clamp the paddle's position to stay within the screen boundaries
        self.rect.y = max(0, min(self.rect.y, config.SCREEN_HEIGHT - self.height))
    def swing(self):
        """Initiates the swing action if the paddle is not already swinging."""
        if not self.is_swinging:
            self.is_swinging = True
            self.swing_timer = self.swing_duration
    def update(self, dt: float):
        """
        Updates the paddle's state, primarily for handling the swing animation.
        This should be called every frame or physics step.
        Args:
            dt (float): The time delta since the last update, in seconds.
        """
        if dt == 0:
            return  # Avoid division by zero if time has not passed
        old_angle = self.angle
        if self.is_swinging:
            self.swing_timer -= dt
            if self.swing_timer <= 0:
                # End of swing animation
                self.is_swinging = False
                self.swing_timer = 0.0
                self.angle = 0.0
            else:
                # Calculate the current angle of the swing using a sine wave
                # for a smooth out-and-back motion.
                progress = 1.0 - (self.swing_timer / self.swing_duration)
                swing_factor = math.sin(progress * math.pi)
                
                self.angle = self.max_swing_angle * swing_factor
                
                # Right paddle swings in the opposite direction (negative angle)
                if self.side == 'right':
                    self.angle *= -1
        else:
            # Ensure angle is zero when not swinging
            self.angle = 0.0
        
        # Calculate angular velocity (degrees per second) for physics calculations
        self.angular_velocity = (self.angle - old_angle) / dt
    def get_corners(self) -> list[np.ndarray]:
        """
        Calculates the world coordinates of the four corners of the rotated paddle.
        This is essential for accurate collision detection and rendering.
        Returns:
            list[np.ndarray]: A list of four 2D numpy arrays, each representing a corner's (x, y) coords.
        """
        center = np.array(self.rect.center)
        half_w = self.width / 2
        half_h = self.height / 2
        # Corner points relative to the paddle's center (0,0) before rotation
        local_corners = [
            np.array([-half_w, -half_h]),  # Top-left
            np.array([ half_w, -half_h]),  # Top-right
            np.array([ half_w,  half_h]),  # Bottom-right
            np.array([-half_w,  half_h]),  # Bottom-left
        ]
        # Create 2D rotation matrix
        rad = math.radians(self.angle)
        cos_a = math.cos(rad)
        sin_a = math.sin(rad)
        rot_matrix = np.array([[cos_a, -sin_a],
                               [sin_a,  cos_a]])
        # Rotate each corner and translate it to its world position by adding the center coordinates
        world_corners = [center + (rot_matrix @ p) for p in local_corners]
        
        return world_corners
    def draw(self, screen: pygame.Surface):
        """
        Draws the paddle on the given screen as a rotated polygon.
        Args:
            screen (pygame.Surface): The Pygame surface to draw on.
        """
        # Get the rotated corners and draw a polygon
        corners = self.get_corners()
        # pygame.draw.polygon expects a list of tuples, so we convert the numpy arrays
        pygame.draw.polygon(screen, self.color, [tuple(p) for p in corners])
    def get_state(self) -> np.ndarray:
        """
        Gets the normalized state of the paddle for the reinforcement learning agent.
        Returns:
            np.ndarray: A numpy array containing the paddle's normalized state
                        [normalized_y_position, is_swinging_flag].
        """
        # Normalize y-position to be between -1 (top) and 1 (bottom) relative to screen center
        norm_y = (self.rect.centery - config.SCREEN_HEIGHT / 2) / (config.SCREEN_HEIGHT / 2)
        
        # Swing state as a binary flag (0.0 for not swinging, 1.0 for swinging)
        swing_state = 1.0 if self.is_swinging else 0.0
        return np.array([norm_y, swing_state], dtype=np.float32)
    
    def get_surface_angle(self):
        """
        Returns the current surface angle of the paddle for collision calculations.
        
        Returns:
            float: The paddle's surface angle in degrees.
        """
        return self.angle
================================================
FILE: pyproject.toml
================================================
[project]
name = "pinpong"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "ffmpeg>=1.4",
    "imageio[ffmpeg,pyav]>=2.37.0",
    "numpy>=2.3.2",
    "pygame>=2.6.1",
    "rich>=14.1.0",
    "torch>=2.7.1",
    "torchvision>=0.22.1",
]

================================================
FILE: vtrace_agent.py
================================================
import os
import random
from collections import deque, namedtuple
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import config
from neural_network import ActorCriticNet
# --- Device Setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
VTraceExperience = namedtuple(
    'VTraceExperience',
    ('state', 'action', 'reward', 'next_state', 'done', 'log_prob')
)
class VTraceAgent:
    """Trust-region actor-critic agent using V-trace off-policy corrections."""
    def __init__(self, agent_id: str):
        self.agent_id = agent_id
        self.state_size = config.STATE_SIZE
        self.action_size = config.ACTION_SIZE
        self.network = ActorCriticNet(self.state_size, self.action_size).to(device)
        self.optimizer = optim.Adam(self.network.parameters(), lr=config.LEARNING_RATE)
        self.memory = deque(maxlen=config.MEMORY_SIZE)
        self.epsilon = config.EPSILON_START
        self.last_log_prob = 0.0
    def choose_action(self, state: np.ndarray) -> int:
        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)
        logits, _ = self.network(state_tensor)
        probs = torch.softmax(logits, dim=1).squeeze(0)
        dist = torch.distributions.Categorical(probs)
        if random.random() < self.epsilon:
            action = random.randrange(self.action_size)
            log_prob = float(torch.log(torch.tensor(1.0 / self.action_size)))
        else:
            action = dist.sample().item()
            log_prob = dist.log_prob(torch.tensor(action, device=device)).item()
        self.last_log_prob = log_prob
        return action
    def remember(self, state, action, reward, next_state, done):
        self.memory.append(VTraceExperience(state, action, reward, next_state, done, self.last_log_prob))
    def learn(self):
        if len(self.memory) < config.BATCH_SIZE:
            return
        experiences = random.sample(self.memory, config.BATCH_SIZE)
        batch = VTraceExperience(*zip(*experiences))
        states = torch.from_numpy(np.vstack(batch.state)).float().to(device)
        actions = torch.tensor(batch.action, dtype=torch.long, device=device)
        rewards = torch.tensor(batch.reward, dtype=torch.float32, device=device)
        next_states = torch.from_numpy(np.vstack(batch.next_state)).float().to(device)
        dones = torch.tensor(batch.done, dtype=torch.float32, device=device)
        behavior_log_probs = torch.tensor(batch.log_prob, dtype=torch.float32, device=device)
        logits, values = self.network(states)
        dist = torch.distributions.Categorical(logits=logits)
        action_log_probs = dist.log_prob(actions)
        entropy = dist.entropy().mean()
        values = values.squeeze()
        with torch.no_grad():
            _, next_values = self.network(next_states)
            next_values = next_values.squeeze()
        rho = torch.exp(action_log_probs - behavior_log_probs)
        rho_bar = torch.clamp(rho, max=config.VTRACE_RHO_CLIP)
        c = torch.clamp(rho, max=config.VTRACE_C_CLIP)
        # Calculate 1-step temporal difference error
        td_error = rewards + config.GAMMA * next_values * (1 - dones) - values
        # V-trace value target uses rho_bar clipped importance ratios
        deltas = rho_bar * td_error
        value_targets = values + deltas
        value_loss = (value_targets.detach() - values).pow(2).mean()
        # V-trace policy gradient advantage uses c clipped importance ratios
        pg_advantage = (c * td_error).detach()
        policy_loss = -(action_log_probs * pg_advantage).mean()
        policy_loss -= config.ENTROPY_BETA * entropy
        loss = policy_loss + 0.5 * value_loss
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 40.0)
        self.optimizer.step()
    def decay_epsilon(self):
        self.epsilon = max(config.EPSILON_END, config.EPSILON_DECAY * self.epsilon)
    def update_target_network(self):
        # Actor-critic does not use a target network; method kept for API parity.
        pass
    def save_model(self, episode: int):
        if not os.path.exists(config.MODEL_PATH):
            os.makedirs(config.MODEL_PATH)
        filename = f"{self.agent_id}_episode_{episode}.pth"
        path = os.path.join(config.MODEL_PATH, filename)
        torch.save(self.network.state_dict(), path)
        print(f"Model for {self.agent_id} saved to {path}")
    def load_model(self, path: str):
        if os.path.exists(path):
            try:
                self.network.load_state_dict(torch.load(path, map_location=device))
                print(f"Model for {self.agent_id} loaded from {path}")
            except Exception as e:
                print(f"Error loading model from {path}. Starting fresh. Error: {e}")
        else:
            print(f"No model found at {path}, starting from scratch.")
